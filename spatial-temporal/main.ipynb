{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1938b0bb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59beaf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from time import time\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb592da2",
   "metadata": {},
   "source": [
    "## Simulation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2079caeb",
   "metadata": {
    "code_folding": [
     0,
     6,
     12,
     112
    ]
   },
   "outputs": [],
   "source": [
    "def my_log(number):\n",
    "    if number == 0:\n",
    "        return -1 * np.infty\n",
    "    return np.log(number)\n",
    "\n",
    "\n",
    "def my_divide(a, b):\n",
    "    if b == 0:\n",
    "        return np.sign(a) * np.infty\n",
    "    return a / b\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, alpha, beta, gamma, n, s, e, i):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.n = n\n",
    "\n",
    "        self.s = s\n",
    "        self.e = e\n",
    "        self.i = i\n",
    "        self.r = n - s - e - i\n",
    "\n",
    "        self.s_to_e_coefficient = beta * i * s / n\n",
    "        self.e_to_i_coefficient = alpha * e\n",
    "        self.i_to_r_coefficient = gamma * i\n",
    "\n",
    "        self.total_time = 0.0\n",
    "        self.history = [[self.total_time, self.s, self.e, self.i, self.r]]\n",
    "\n",
    "        self.next_unit_time_step = 1\n",
    "        self.unit_time_history = [[0, self.s, self.e, self.i, self.r]]\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.unit_time_history\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.unit_time_history[i]\n",
    "\n",
    "    def update_coefficients(self):\n",
    "        self.s_to_e_coefficient = self.beta * self.i * self.s / self.n\n",
    "        self.e_to_i_coefficient = self.alpha * self.e\n",
    "        self.i_to_r_coefficient = self.gamma * self.i\n",
    "\n",
    "    def update_seir(self, argmin):\n",
    "        if argmin == 0:\n",
    "            self.s -= 1\n",
    "            self.e += 1\n",
    "        elif argmin == 1:\n",
    "            self.e -= 1\n",
    "            self.i += 1\n",
    "        else:\n",
    "            self.i -= 1\n",
    "            self.r += 1\n",
    "\n",
    "    def time_step(self):\n",
    "        self.update_coefficients()\n",
    "\n",
    "        if max(self.s_to_e_coefficient, self.e_to_i_coefficient, self.i_to_r_coefficient) == 0:\n",
    "            self.total_time += 0.1\n",
    "            self.history.append([self.total_time, self.s, self.e, self.i, self.r])\n",
    "            return\n",
    "\n",
    "        s_to_e_time = -1 * my_divide(my_log(random.uniform(0, 1)), self.s_to_e_coefficient)\n",
    "        e_to_i_time = -1 * my_divide(my_log(random.uniform(0, 1)), self.e_to_i_coefficient)\n",
    "        i_to_r_time = -1 * my_divide(my_log(random.uniform(0, 1)), self.i_to_r_coefficient)\n",
    "\n",
    "        time_array = np.asarray([s_to_e_time, e_to_i_time, i_to_r_time])\n",
    "        self.total_time += min(time_array)\n",
    "        argmin = np.argmin(time_array)\n",
    "        self.update_seir(argmin)\n",
    "        self.history.append([self.total_time, self.s, self.e, self.i, self.r])\n",
    "\n",
    "    def graph(self):\n",
    "        history = np.asarray(self.unit_time_history)\n",
    "\n",
    "        time_data = history[:, 0]\n",
    "        s_data = history[:, 1]\n",
    "        e_data = history[:, 2]\n",
    "        i_data = history[:, 3]\n",
    "        r_data = history[:, 4]\n",
    "\n",
    "        plt.plot(time_data, s_data, label='Susceptible subjects')\n",
    "        plt.plot(time_data, e_data, label='Exposed subjects')\n",
    "        plt.plot(time_data, i_data, label='Infected subjects')\n",
    "        plt.plot(time_data, r_data, label='Removed subjects')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def simulate_single_time_unit(self):\n",
    "        while self.total_time <= self.next_unit_time_step:\n",
    "            self.time_step()\n",
    "\n",
    "        self.history.pop()\n",
    "        self.total_time = self.next_unit_time_step\n",
    "\n",
    "        unit_data_to_add = [self.next_unit_time_step]\n",
    "        unit_data_to_add.extend(self.history[-1][1:])\n",
    "\n",
    "        self.unit_time_history.append(unit_data_to_add)\n",
    "        self.history.append(unit_data_to_add)  # can do this due to lack of memory of Poisson processes\n",
    "        self.s, self.e, self.i, self.r = \\\n",
    "            self.history[-1][1], self.history[-1][2], self.history[-1][3], self.history[-1][4]\n",
    "        self.next_unit_time_step += 1\n",
    "\n",
    "    def simulate(self):\n",
    "        while self.unit_time_history[-1][2] + self.unit_time_history[-1][3] != 0:\n",
    "            self.simulate_single_time_unit()\n",
    "\n",
    "\n",
    "class Simulation:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "        self.number_of_nodes = 0\n",
    "        self.diffusion_matrix = np.empty((0, 0))\n",
    "        self.current_time_step = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nodes[0].unit_time_history)\n",
    "\n",
    "    def add_node(self, node):\n",
    "        self.nodes.append(node)\n",
    "        self.number_of_nodes += 1\n",
    "        padding = ((0, 1), (0, 1))\n",
    "        self.diffusion_matrix = np.pad(self.diffusion_matrix,\n",
    "                                       pad_width=padding,\n",
    "                                       mode='constant',\n",
    "                                       constant_values=0.0)\n",
    "\n",
    "    def populate_diffusion_matrix(self):\n",
    "        for node_index in range(self.number_of_nodes):\n",
    "            for index in range(self.number_of_nodes):\n",
    "                minimal_n = min(self.nodes[node_index].n, self.nodes[index].n)\n",
    "                if index == node_index:\n",
    "                    continue\n",
    "                diffusion_number = int(1 * random.uniform(0.5, 1) * minimal_n /\n",
    "                                       (self.number_of_nodes * 4 ** (abs(node_index - index))))\n",
    "                self.diffusion_matrix[node_index][index] = diffusion_number\n",
    "                self.diffusion_matrix[index][node_index] = diffusion_number\n",
    "\n",
    "    def get_accumulated_seir_from_diff_matrix(self, node_index):\n",
    "        accumulated_seir = [0, 0, 0, 0]\n",
    "\n",
    "        for index in range(self.number_of_nodes):\n",
    "            total_diff_number = self.diffusion_matrix[node_index][index]\n",
    "            node = self.nodes[index]\n",
    "\n",
    "            s_transfer_in = int(total_diff_number * node.s / node.n)\n",
    "            e_transfer_in = int(total_diff_number * node.e / node.n)\n",
    "            i_transfer_in = int(total_diff_number * node.i / node.n)\n",
    "            r_transfer_in = total_diff_number - s_transfer_in - e_transfer_in - i_transfer_in\n",
    "\n",
    "            accumulated_seir[0] += s_transfer_in\n",
    "            accumulated_seir[1] += e_transfer_in\n",
    "            accumulated_seir[2] += i_transfer_in\n",
    "            accumulated_seir[3] += r_transfer_in\n",
    "\n",
    "        for index in range(self.number_of_nodes):\n",
    "            total_diff_number = self.diffusion_matrix[node_index][index]\n",
    "            node = self.nodes[node_index]\n",
    "\n",
    "            s_transfer_out = int(total_diff_number * node.s / node.n)\n",
    "            e_transfer_out = int(total_diff_number * node.e / node.n)\n",
    "            i_transfer_out = int(total_diff_number * node.i / node.n)\n",
    "            r_transfer_out = total_diff_number - s_transfer_out - e_transfer_out - i_transfer_out\n",
    "\n",
    "            accumulated_seir[0] -= s_transfer_out\n",
    "            accumulated_seir[1] -= e_transfer_out\n",
    "            accumulated_seir[2] -= i_transfer_out\n",
    "            accumulated_seir[3] -= r_transfer_out\n",
    "\n",
    "        return accumulated_seir\n",
    "\n",
    "    def diffuse(self):\n",
    "        accumulated_seir_dic = {}\n",
    "\n",
    "        for node_index in range(self.number_of_nodes):\n",
    "            accumulated_seir_dic[node_index] = self.get_accumulated_seir_from_diff_matrix(node_index)\n",
    "\n",
    "        for node_index in range(self.number_of_nodes):\n",
    "            self.nodes[node_index].s += accumulated_seir_dic[node_index][0]\n",
    "            self.nodes[node_index].e += accumulated_seir_dic[node_index][1]\n",
    "            self.nodes[node_index].i += accumulated_seir_dic[node_index][2]\n",
    "            self.nodes[node_index].r += accumulated_seir_dic[node_index][3]\n",
    "\n",
    "    def simulate_single_time_unit(self):\n",
    "        #  print('Simulating time step ' + str(self.current_time_step) + '.')\n",
    "        for node in self.nodes:\n",
    "            node.simulate_single_time_unit()\n",
    "        self.diffuse()\n",
    "        self.current_time_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6e35c",
   "metadata": {},
   "source": [
    "## Model Classes Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe598d2",
   "metadata": {
    "code_folding": [
     0,
     7,
     11,
     16,
     17,
     143,
     144,
     276,
     277,
     367,
     368
    ]
   },
   "outputs": [],
   "source": [
    "def vectorize(sim):\n",
    "    vec = sim[0]\n",
    "    for node_data in sim[1:]:\n",
    "        vec = np.append(vec, node_data, axis=1)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def un_vectorize(sim, num_feats):\n",
    "    pass\n",
    "\n",
    "\n",
    "def normalize_sim(sim):\n",
    "    max_pop = max([sum(node[0]) for node in sim])\n",
    "    return np.asarray(sim, dtype=float) / max_pop\n",
    "\n",
    "\n",
    "class AbstractRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 previous_steps,\n",
    "                 future_steps,\n",
    "                 hidden_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.history = {'epochs': [], 'training_loss': [], 'validation_loss': []}\n",
    "        self.previous_steps = previous_steps\n",
    "        self.future_steps = future_steps\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initialize(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initial_hidden(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_datasets(self, data_file, stride, train_num, val_num):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def calculate_loss(self, data, loss_func):\n",
    "        losses = []\n",
    "\n",
    "        for i, (x, y) in enumerate(data):\n",
    "            output = None\n",
    "            hidden_state = self.initial_hidden()\n",
    "            for vector in x:\n",
    "                output, hidden_state = self.forward(vector, hidden_state)\n",
    "            loss = loss_func(output, y)\n",
    "            losses.append(loss.item())\n",
    "        avg_loss = np.asarray(losses).mean()\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def train_model(self,\n",
    "                    data_file,\n",
    "                    train_num,\n",
    "                    val_num,\n",
    "                    stride,\n",
    "                    epochs,\n",
    "                    loss_func=nn.MSELoss,\n",
    "                    optim=torch.optim.Adam,\n",
    "                    lr=0.001,\n",
    "                    lr_decay=1.0,\n",
    "                    batch_size=1):\n",
    "        pop = sum(np.load(data_file)[0][0][0])\n",
    "        training_data, validation_data = self.create_datasets(data_file,\n",
    "                                                              stride,\n",
    "                                                              train_num,\n",
    "                                                              val_num)\n",
    "\n",
    "        pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print('Total number of trainable parameters:', pytorch_total_params)\n",
    "\n",
    "        loss_func = loss_func()\n",
    "        learning_rate = lr\n",
    "        t1 = time()\n",
    "        for epoch in range(epochs):\n",
    "            opt = optim(self.parameters(), lr=learning_rate)\n",
    "            random.shuffle(training_data)\n",
    "            losses = []\n",
    "\n",
    "            print('Epoch ' + str(epoch + 1) + '/' + str(epochs) + ': ', end='')\n",
    "            total_equals = 0\n",
    "\n",
    "            loss = torch.tensor(0.0, dtype=torch.float, requires_grad=True)\n",
    "            for i, (x, y) in enumerate(training_data):\n",
    "                equals_to_print = int(40 * (i + 1) / len(training_data)) - total_equals\n",
    "                total_equals += equals_to_print\n",
    "                output = None\n",
    "                hidden_state = self.initial_hidden()\n",
    "\n",
    "                for vector in x:\n",
    "                    output, hidden_state = self.forward(vector, hidden_state)\n",
    "\n",
    "                loss = loss + loss_func(output, y)\n",
    "\n",
    "                if i > 0 and i % batch_size == 0:\n",
    "                    loss = loss / batch_size\n",
    "                    opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    loss = torch.tensor(0.0, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                for _ in range(equals_to_print):\n",
    "                    print('=', end='')\n",
    "            with torch.no_grad():\n",
    "                train_loss = self.calculate_loss(training_data, loss_func)\n",
    "                train_loss = np.sqrt(train_loss) * pop\n",
    "            print(' Done. ', end='')\n",
    "            print('Training error: ' + '{:.2f}'.format(train_loss) + '. ', end='')\n",
    "            with torch.no_grad():\n",
    "                val_loss = self.calculate_loss(validation_data, loss_func)\n",
    "                val_loss = np.sqrt(val_loss) * pop\n",
    "            print('Validation error: ' + '{:.2f}'.format(val_loss) + '. ', end='')\n",
    "            print('Learning Rate: ' + '{:.3e}'.format(learning_rate) + '. ', end='')\n",
    "            self.history['epochs'].append(epoch + 1)\n",
    "            self.history['training_loss'].append(train_loss)\n",
    "            self.history['validation_loss'].append(val_loss)\n",
    "            learning_rate *= lr_decay\n",
    "            t2 = time()\n",
    "            total_time = t2 - t1\n",
    "            epochs_left = epochs - epoch - 1\n",
    "            avg_time = total_time / (epoch + 1)\n",
    "            time_left = epochs_left * avg_time\n",
    "            print('Time left: ' + str(timedelta(seconds=time_left)))\n",
    "\n",
    "    def predict(self, x, future_steps):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def plot_loss(self, start_epoch):\n",
    "        epochs = self.history['epochs'][start_epoch - 1:]\n",
    "        train_loss = self.history['training_loss'][start_epoch - 1:]\n",
    "        val_loss = self.history['validation_loss'][start_epoch - 1:]\n",
    "        plt.plot(epochs, train_loss, label='Training loss')\n",
    "        plt.plot(epochs, val_loss, label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class AbstractLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 previous_steps,\n",
    "                 future_steps,\n",
    "                 hidden_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.history = {'epochs': [], 'training_loss': [], 'validation_loss': []}\n",
    "        self.previous_steps = previous_steps\n",
    "        self.future_steps = future_steps\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initialize(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initial_hidden(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initial_cell(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_datasets(self, data_file, stride, train_num, val_num):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def calculate_loss(self, data, loss_func):\n",
    "        losses = []\n",
    "\n",
    "        for i, (x, y) in enumerate(data):\n",
    "            output = None\n",
    "            hidden_state = self.initial_hidden()\n",
    "            cell_state = self.initial_cell()\n",
    "            for vector in x:\n",
    "                output, hidden_state, cell_state = self.forward(vector, hidden_state, cell_state)\n",
    "            loss = loss_func(output, y)\n",
    "            losses.append(loss.item())\n",
    "        avg_loss = np.asarray(losses).mean()\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def train_model(self,\n",
    "                    data_file,\n",
    "                    train_num,\n",
    "                    val_num,\n",
    "                    stride,\n",
    "                    epochs,\n",
    "                    loss_func=nn.MSELoss,\n",
    "                    optim=torch.optim.Adam,\n",
    "                    lr=0.001,\n",
    "                    lr_decay=1.0,\n",
    "                    batch_size=1):\n",
    "\n",
    "        pop = sum(np.load(data_file)[0][0][0])\n",
    "\n",
    "        training_data, validation_data = self.create_datasets(data_file,\n",
    "                                                              stride,\n",
    "                                                              train_num,\n",
    "                                                              val_num)\n",
    "\n",
    "        pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print('Total number of trainable parameters:', pytorch_total_params)\n",
    "\n",
    "        loss_func = loss_func()\n",
    "        learning_rate = lr\n",
    "        t1 = time()\n",
    "        for epoch in range(epochs):\n",
    "            opt = optim(self.parameters(), lr=learning_rate)\n",
    "            random.shuffle(training_data)\n",
    "            losses = []\n",
    "\n",
    "            print('Epoch ' + str(epoch + 1) + '/' + str(epochs) + ': ', end='')\n",
    "            total_equals = 0\n",
    "\n",
    "            loss = torch.tensor(0.0, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "            for i, (x, y) in enumerate(training_data):\n",
    "                equals_to_print = int(40 * (i + 1) / len(training_data)) - total_equals\n",
    "                total_equals += equals_to_print\n",
    "                output = None\n",
    "                hidden_state = self.initial_hidden()\n",
    "                cell_state = self.initial_cell()\n",
    "                for vector in x:\n",
    "                    output, hidden_state, cell_state = self.forward(vector, hidden_state, cell_state)\n",
    "\n",
    "                loss = loss + loss_func(output, y)\n",
    "\n",
    "                if i > 0 and i % batch_size == 0:\n",
    "                    loss = loss / batch_size\n",
    "                    opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    loss = torch.tensor(0.0, dtype=torch.float, requires_grad=True)\n",
    "                losses.append(loss.item())\n",
    "                for _ in range(equals_to_print):\n",
    "                    print('=', end='')\n",
    "            with torch.no_grad():\n",
    "                train_loss = self.calculate_loss(training_data, loss_func)\n",
    "                train_loss = np.sqrt(train_loss) * pop\n",
    "            print(' Done. ', end='')\n",
    "            print('Training error: ' + '{:.2f}'.format(train_loss) + '. ', end='')\n",
    "            with torch.no_grad():\n",
    "                val_loss = self.calculate_loss(validation_data, loss_func)\n",
    "                val_loss = np.sqrt(val_loss) * pop\n",
    "            print('Validation error: ' + '{:.2f}'.format(val_loss) + '. ', end='')\n",
    "            print('Learning Rate: ' + '{:.3e}'.format(learning_rate) + '. ', end='')\n",
    "            self.history['epochs'].append(epoch + 1)\n",
    "            self.history['training_loss'].append(train_loss)\n",
    "            self.history['validation_loss'].append(val_loss)\n",
    "            learning_rate *= lr_decay\n",
    "            t2 = time()\n",
    "            total_time = t2 - t1\n",
    "            epochs_left = epochs - epoch - 1\n",
    "            avg_time = total_time / (epoch + 1)\n",
    "            time_left = epochs_left * avg_time\n",
    "            print('Time left: ' + str(timedelta(seconds=time_left)))\n",
    "\n",
    "    def predict(self, x, future_steps):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def plot_loss(self, start_epoch):\n",
    "        epochs = self.history['epochs'][start_epoch - 1:]\n",
    "        train_loss = self.history['training_loss'][start_epoch - 1:]\n",
    "        val_loss = self.history['validation_loss'][start_epoch - 1:]\n",
    "        plt.plot(epochs, train_loss, label='Training loss')\n",
    "        plt.plot(epochs, val_loss, label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "class GCRNN(AbstractRNN):\n",
    "    def __init__(self, num_nodes, num_feats, previous_steps, future_steps, hidden_size):\n",
    "        super().__init__(previous_steps,\n",
    "                         future_steps,\n",
    "                         hidden_size)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_feats = num_feats\n",
    "        self.edge_index = None\n",
    "        self.make_edge_index()\n",
    "\n",
    "        self.graph_conv = pyg.nn.GCNConv(num_feats + hidden_size, hidden_size)\n",
    "        self.b_matrix = nn.Parameter(torch.zeros(num_nodes, hidden_size))\n",
    "        self.v_matrix = nn.Parameter(torch.zeros(hidden_size, num_feats))\n",
    "        self.c_matrix = nn.Parameter(torch.zeros(num_nodes, num_feats))\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def make_edge_index(self):\n",
    "        first_vec = []\n",
    "        second_vec = []\n",
    "        for i in range(self.num_nodes):\n",
    "            first_vec.append(i)\n",
    "            first_vec.append(i)\n",
    "        first_vec = first_vec[1:-1]\n",
    "        for i in range(len(first_vec)):\n",
    "            second_vec.append(first_vec[i] + ((-1) ** i))\n",
    "        e_i = np.asarray([first_vec, second_vec])\n",
    "        self.edge_index = torch.tensor(e_i, dtype=torch.long)\n",
    "\n",
    "    def create_dataset_from_sim(self, sim, stride):\n",
    "        sim_length = len(sim[0])\n",
    "        graph_data = normalize_sim(sim)\n",
    "\n",
    "        graph_series = []\n",
    "\n",
    "        for time in range(sim_length):\n",
    "            vertices = []\n",
    "            for node_index in range(len(graph_data)):\n",
    "                seir = graph_data[node_index][time]\n",
    "                vertices.append(seir)\n",
    "            vertices = np.asarray(vertices)\n",
    "            vertices = torch.tensor(vertices, dtype=torch.float)\n",
    "            graph = Data(vertices, self.edge_index)\n",
    "            graph_series.append(graph)\n",
    "\n",
    "        graph_dataset = []\n",
    "        start = random.randint(0, stride - 1)\n",
    "\n",
    "        for index in range(start, sim_length - self.previous_steps - self.future_steps + 1, stride):\n",
    "            datum = graph_series[index: index + self.previous_steps]\n",
    "            label = graph_series[index + self.previous_steps].x\n",
    "            graph_dataset.append((datum, label))\n",
    "\n",
    "        return graph_dataset\n",
    "\n",
    "    def create_datasets(self, datafile, stride, train_num, val_num):\n",
    "\n",
    "        train_dataset = []\n",
    "        val_dataset = []\n",
    "        all_data = np.load(datafile)\n",
    "\n",
    "        for i in range(train_num):\n",
    "            mini_set = self.create_dataset_from_sim(all_data[i], stride)\n",
    "            train_dataset.extend(mini_set)\n",
    "\n",
    "        for i in range(train_num, train_num + val_num):\n",
    "            mini_set = self.create_dataset_from_sim(all_data[i], stride)\n",
    "            val_dataset.extend(mini_set)\n",
    "\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        x_h_concatenated = torch.cat((x.x, hidden_state), dim=1)\n",
    "        conv = self.graph_conv(x_h_concatenated, x.edge_index)\n",
    "        h_pre = self.b_matrix + conv\n",
    "        new_hidden = torch.sigmoid(h_pre)\n",
    "        o = self.c_matrix + torch.mm(new_hidden, self.v_matrix)\n",
    "        return o, new_hidden\n",
    "\n",
    "    def initialize(self):\n",
    "        torch.nn.init.xavier_uniform_(self.v_matrix)\n",
    "\n",
    "    def initial_hidden(self):\n",
    "        return torch.zeros(self.num_nodes, self.hidden_size)\n",
    "\n",
    "    def predict(self, sim, time_steps):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class GCLSTM(AbstractLSTM):\n",
    "    def __init__(self, num_nodes, num_feats, previous_steps, future_steps, hidden_size):\n",
    "        super().__init__(previous_steps,\n",
    "                         future_steps,\n",
    "                         hidden_size)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_feats = num_feats\n",
    "        self.edge_index = None\n",
    "        self.make_edge_index()\n",
    "\n",
    "        self.gf = pyg.nn.GCNConv(num_feats + hidden_size, hidden_size)\n",
    "        self.gi = pyg.nn.GCNConv(num_feats + hidden_size, hidden_size)\n",
    "        self.go = pyg.nn.GCNConv(num_feats + hidden_size, hidden_size)\n",
    "        self.gc = pyg.nn.GCNConv(num_feats + hidden_size, hidden_size)\n",
    "\n",
    "        self.bf = nn.Parameter(torch.zeros(num_nodes, hidden_size))\n",
    "        self.bi = nn.Parameter(torch.zeros(num_nodes, hidden_size))\n",
    "        self.bo = nn.Parameter(torch.zeros(num_nodes, hidden_size))\n",
    "        self.bc = nn.Parameter(torch.zeros(num_nodes, hidden_size))\n",
    "\n",
    "        self.v = nn.Parameter(torch.zeros(hidden_size, num_feats))\n",
    "        self.c = nn.Parameter(torch.zeros(num_nodes, num_feats))\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def make_edge_index(self):\n",
    "        first_vec = []\n",
    "        second_vec = []\n",
    "        for i in range(self.num_nodes):\n",
    "            first_vec.append(i)\n",
    "            first_vec.append(i)\n",
    "        first_vec = first_vec[1:-1]\n",
    "        for i in range(len(first_vec)):\n",
    "            second_vec.append(first_vec[i] + ((-1) ** i))\n",
    "        e_i = np.asarray([first_vec, second_vec])\n",
    "        self.edge_index = torch.tensor(e_i, dtype=torch.long)\n",
    "\n",
    "    def create_dataset_from_sim(self, sim, stride):\n",
    "        sim_length = len(sim[0])\n",
    "        graph_data = normalize_sim(sim)\n",
    "\n",
    "        graph_series = []\n",
    "\n",
    "        for time in range(sim_length):\n",
    "            vertices = []\n",
    "            for node_index in range(len(graph_data)):\n",
    "                seir = graph_data[node_index][time]\n",
    "                vertices.append(seir)\n",
    "            vertices = np.asarray(vertices)\n",
    "            vertices = torch.tensor(vertices, dtype=torch.float)\n",
    "            graph = Data(vertices, self.edge_index)\n",
    "            graph_series.append(graph)\n",
    "\n",
    "        graph_dataset = []\n",
    "        start = random.randint(0, stride - 1)\n",
    "\n",
    "        for index in range(start, sim_length - self.previous_steps - self.future_steps + 1, stride):\n",
    "            datum = graph_series[index: index + self.previous_steps]\n",
    "            label = graph_series[index + self.previous_steps].x\n",
    "            graph_dataset.append((datum, label))\n",
    "\n",
    "        return graph_dataset\n",
    "\n",
    "    def create_datasets(self, datafile, stride, train_num, val_num):\n",
    "\n",
    "        train_dataset = []\n",
    "        val_dataset = []\n",
    "        all_data = np.load(datafile)\n",
    "\n",
    "        for i in range(train_num):\n",
    "            mini_set = self.create_dataset_from_sim(all_data[i], stride)\n",
    "            train_dataset.extend(mini_set)\n",
    "\n",
    "        for i in range(train_num, train_num + val_num):\n",
    "            mini_set = self.create_dataset_from_sim(all_data[i], stride)\n",
    "            val_dataset.extend(mini_set)\n",
    "\n",
    "        print(len(train_dataset))\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "\n",
    "        xh = torch.cat((x.x, hidden_state), dim=1)\n",
    "\n",
    "        ft = torch.sigmoid(self.bf + self.gf(xh, x.edge_index))\n",
    "        it = torch.sigmoid(self.bi + self.gi(xh, x.edge_index))\n",
    "        ot = torch.sigmoid(self.bo + self.go(xh, x.edge_index))\n",
    "        ct_prime = torch.tanh(self.bc + self.gc(xh, x.edge_index))\n",
    "\n",
    "        new_cell = (ft * cell_state) + (it * ct_prime)\n",
    "        new_hidden = ot * torch.tanh(new_cell)\n",
    "\n",
    "        real_output = torch.mm(ot, self.v) + self.c\n",
    "\n",
    "        return real_output, new_hidden, new_cell\n",
    "\n",
    "    def initialize(self):\n",
    "        torch.nn.init.xavier_uniform_(self.v)\n",
    "\n",
    "    def initial_hidden(self):\n",
    "        return torch.zeros(self.num_nodes, self.hidden_size)\n",
    "\n",
    "    def initial_cell(self):\n",
    "        return torch.zeros(self.num_nodes, self.hidden_size)\n",
    "\n",
    "    def predict(self, sim, time_steps):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371419ec",
   "metadata": {},
   "source": [
    "## Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b3054",
   "metadata": {
    "code_folding": [
     0,
     9,
     17,
     26,
     46,
     67,
     82,
     97
    ]
   },
   "outputs": [],
   "source": [
    "def unvectorize(sim, num_nodes):\n",
    "    sim = np.asarray(sim)\n",
    "    ret = []\n",
    "    for i in range(num_nodes):\n",
    "        node_data = sim[:, i * 4:i * 4 + 4]\n",
    "        ret.append(node_data)\n",
    "    return np.asarray(ret)\n",
    "\n",
    "\n",
    "def rnn_predict_one_step(model, input_steps):\n",
    "    hidden_state = model.initial_hidden()\n",
    "    output = None\n",
    "    for vector in input_steps:\n",
    "        output, hidden_state = model(vector, hidden_state)\n",
    "    return output\n",
    "\n",
    "\n",
    "def lstm_predict_one_step(model, input_steps):\n",
    "    hidden_state = model.initial_hidden()\n",
    "    cell_state = model.initial_hidden()\n",
    "    output = None\n",
    "    for vector in input_steps:\n",
    "        output, hidden_state, cell_state = model(vector, hidden_state, cell_state)\n",
    "    return output\n",
    "\n",
    "\n",
    "def rnn_predict(model, sim, prev_steps, time_steps, num_nodes):\n",
    "    model.eval()\n",
    "    max_pop = max([sum(node[0]) for node in sim])\n",
    "    sim = normalize_sim(sim)\n",
    "    sim = vectorize(sim)\n",
    "    sim = torch.tensor(sim, dtype=torch.float)\n",
    "    input_steps = sim[:prev_steps]\n",
    "    current_data = input_steps\n",
    "    future_data = []\n",
    "\n",
    "    for step in range(time_steps):\n",
    "        next_step = rnn_predict_one_step(model, current_data)\n",
    "        future_data.append(next_step.detach().numpy())\n",
    "        next_step = next_step[None, :]\n",
    "        current_data = torch.cat((current_data, next_step), dim=0)\n",
    "        current_data = current_data[1:]\n",
    "\n",
    "    return unvectorize(future_data, num_nodes) * max_pop\n",
    "\n",
    "\n",
    "def lstm_predict(model, sim, prev_steps, time_steps, num_nodes):\n",
    "    model.eval()\n",
    "    max_pop = max([sum(node[0]) for node in sim])\n",
    "    sim = normalize_sim(sim)\n",
    "    sim = vectorize(sim)\n",
    "    sim = torch.tensor(sim, dtype=torch.float)\n",
    "    input_steps = sim[:prev_steps]\n",
    "    current_data = input_steps\n",
    "\n",
    "    future_data = []\n",
    "\n",
    "    for step in range(time_steps):\n",
    "        next_step = lstm_predict_one_step(model, current_data)\n",
    "        future_data.append(next_step.detach().numpy())\n",
    "        next_step = next_step[None, :]\n",
    "        current_data = torch.cat((current_data, next_step), dim=0)\n",
    "        current_data = current_data[1:]\n",
    "\n",
    "    return unvectorize(future_data, num_nodes) * max_pop\n",
    "\n",
    "\n",
    "def graph_compare_rnn(model, sim, prev_steps, time_steps, num_nodes):\n",
    "    real_data = sim[:, :time_steps + prev_steps]\n",
    "    predicted_data = rnn_predict(model, sim, prev_steps, time_steps, num_nodes)\n",
    "    for node_dex in range(num_nodes):\n",
    "        plt.plot(real_data[node_dex][:prev_steps + time_steps, 2], c='black', lw=1, label='Ground truth')\n",
    "        plt.plot(range(prev_steps, prev_steps + time_steps), predicted_data[node_dex][:, 2],\n",
    "                 ls='dotted', lw=2, c='red', label='Predicted values')\n",
    "        title = 'RNN prediction given data from days 1-' + str(prev_steps)\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Days')\n",
    "        plt.ylabel('Number of infected subjects')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def graph_compare_lstm(model, sim, prev_steps, time_steps, num_nodes):\n",
    "    real_data = sim[:, :time_steps + prev_steps]\n",
    "    predicted_data = lstm_predict(model, sim, prev_steps, time_steps, num_nodes)\n",
    "    for node_dex in range(num_nodes):\n",
    "        plt.plot(real_data[node_dex][:prev_steps + time_steps, 2], c='black', lw=1, label='Ground truth')\n",
    "        plt.plot(range(prev_steps, prev_steps + time_steps), predicted_data[node_dex][:, 2],\n",
    "                 ls='dotted', lw=2, c='red', label='Predicted values')\n",
    "        title = 'RNN prediction given data from days 1-' + str(prev_steps)\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Days')\n",
    "        plt.ylabel('Number of infected subjects')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def graph_compare_rnn_lstm(rnn_model, lstm_model, sim, prev_steps, time_steps, num_nodes, num_sims):\n",
    "    if num_nodes == 1:\n",
    "        rows, cols = 1, 1\n",
    "    elif num_nodes == 2:\n",
    "        rows, cols = 1, 2\n",
    "    elif num_nodes == 10:\n",
    "        rows, cols = 2, 5\n",
    "    elif num_nodes == 20:\n",
    "        rows, cols = 4, 5\n",
    "    else:\n",
    "        rows, cols = 1, num_nodes\n",
    "\n",
    "    real_data = sim[:, :time_steps + prev_steps]\n",
    "    predicted_data_rnn = rnn_predict(rnn_model, sim, prev_steps, time_steps, num_nodes)\n",
    "    predicted_data_lstm = lstm_predict(lstm_model, sim, prev_steps, time_steps, num_nodes)\n",
    "\n",
    "    range_sims = []\n",
    "\n",
    "    for simnum in range(num_sims):\n",
    "        sim = Simulation()\n",
    "        for node in range(num_nodes):\n",
    "            last_step = real_data[node][prev_steps - 1]\n",
    "            s = last_step[0]\n",
    "            e = last_step[1]\n",
    "            i = last_step[2]\n",
    "            r = last_step[3]\n",
    "            n = s + e + i + r\n",
    "            node = Node(0.1, 0.4, 0.05, n, s, e, i)\n",
    "            sim.add_node(node)\n",
    "\n",
    "        for _ in range(time_steps):\n",
    "            sim.simulate_single_time_unit()\n",
    "\n",
    "        fut = []\n",
    "        for node in range(num_nodes):\n",
    "            future = np.asarray(sim.nodes[node].unit_time_history)\n",
    "            fut.append(future)\n",
    "\n",
    "        range_sims.append(fut)\n",
    "\n",
    "    range_sims = np.asarray(range_sims)\n",
    "\n",
    "    fig = plt.figure(1, figsize=(10, 8))\n",
    "\n",
    "    for node_dex in range(num_nodes):\n",
    "        ax = fig.add_subplot(rows, cols, node_dex + 1)\n",
    "\n",
    "        for simul in range_sims:\n",
    "            ax.plot(range(prev_steps - 1, prev_steps + time_steps), simul[node_dex][:, 3],  c='yellow', alpha=0.1)\n",
    "\n",
    "        if node_dex == 0:\n",
    "            ax.plot(0, lw=1, c='black', label='Ground truth')\n",
    "            ax.plot(0, ls='dotted', lw=2, c='red', label='RNN predicted values')\n",
    "            ax.plot(0, ls='dotted', lw=2, c='green', label='LSTM predicted values')\n",
    "            ax.plot(0, c='yellow', label='Range')\n",
    "\n",
    "        ax.plot(real_data[node_dex][:prev_steps + time_steps, 2], c='black', lw=1)\n",
    "\n",
    "        ax.plot(range(prev_steps, prev_steps + time_steps), predicted_data_rnn[node_dex][:, 2],\n",
    "                 ls='dotted', lw=2, c='red')\n",
    "        ax.plot(range(prev_steps, prev_steps + time_steps), predicted_data_lstm[node_dex][:, 2],\n",
    "                 ls='dotted', lw=2, c='green')\n",
    "\n",
    "        ax.set_title('Node ' + str(node_dex + 1))\n",
    "        ax.set(xlabel='Days', ylabel='Infected subjects')\n",
    "\n",
    "    title = 'RNN and LSTM predictions given data from days 1-' + str(prev_steps)\n",
    "    plt.suptitle(title)\n",
    "    fig.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f785cab",
   "metadata": {},
   "source": [
    "## Training Dataset Creation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e617e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(file, window_size, windows_per_sim):\n",
    "    #  Brian's code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318e861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
